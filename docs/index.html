<!doctype html>
<html lang="en">


<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>
        Diversity-regularized Collaborative Exploration</title>

    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
</head>
<!-- === Header Ends === -->


<body>

<script type="text/javascript" src="http://tajs.qq.com/stats?sId=66566800"
        charset="UTF-8"></script>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->


    <div class="header">


        <div class="logo">
            <a href="https://decisionforce.github.io/" target="_blank"><img
                    src="./assets/deciforce.png"></a>
        </div>

        <div class="title" style="padding-top: 25pt;">
            <!-- Set padding as 10 if title is with two lines. -->
            Non-local Policy Optimization via <br>
            Diversity-regularized Collaborative Exploration

        </div>

    </div>

    <!-- === Title Ends === -->
    <div class="author">
        <a href="https://pengzhenghao.github.io" target="_blank">Zhenghao
            Peng</a>,&nbsp;
        <a href="#" target="_blank">Hao Sun</a>,&nbsp;
        <a href="http://bzhou.ie.cuhk.edu.hk" target="_blank">Bolei Zhou</a>
    </div>
    <div class="institution">
        The Chinese University of Hong Kong
    </div>
    <div class="link">
        <a href="https://arxiv.org/pdf/2006.07781.pdf"
           target="_blank">[Paper]</a>&nbsp;
        <a href="https://github.com/decisionforce/DiCE"
           target="_blank">[Code]</a>
    </div>

    <div style="position: relative;">
        <div style="position:absolute;right:0;bottom: 0;">
            <a href="index.html" target="_self">[EN]</a>&nbsp;
            <a href="index_zh.html" target="_self">[中文]</a>
        </div>
    </div>
</div>

<!-- === Home Section Ends === -->


<!-- === Overview Section Starts === -->
<div class="section">
    <div class="title">Overview</div>
    <div class="body">

        <div class="teaser">
            <img align=right style="width: 50%" src="assets/github-teaser.png">
        </div>

        <p>Working together in a team towards a common goal makes life easier.
            However, in most of the existing Reinforcement Learning (RL)
            algorithms, usually only one agent or a global agent with several
            replicas explore the environment and learn to solve the task. The
            agent usually limits its exploration within a small region of the
            state-action space due to the initialization and previous
            experience, as illustrated by the light area in the above figure,
            which we called the <strong>local exploration</strong> problem.</p>

        <p>We address the local exploration problem with a new policy
            optimization framework called Diversity-regularized Collaborative
            Exploration (DiCE). DiCE combines the Collaborative Exploration (CE)
            that maintains a team of agents and shares knowledge across multiple
            agents as well as the
            Diversity Regularization (DR) that directs the exploration of each
            agent and maintains the diversity among them.
            DiCE is implemented in both on-policy and off-policy settings and is
            compared with baselines e.g. PPO and SAC. The experimental results
            show that DiCE outperforms both on-policy and off-policy baselines
            in most cases in the MuJoCo locomotion benchmarks.</p>

    </div>
</div>
<!-- === Overview Section Ends === -->


<!-- === Result Section Starts === -->
<div class="section">
    <div class="title">Results</div>
    <div class="body">

        <table width="100%" style="margin: 0 0; text-align: center;">
            <tr>
                <td>
                    <video style="display:block; width:98%; height:auto;"
                           autoplay="autoplay" controls muted loop="loop">
                        <source src="assets/dice-webpage-video-1080p.m4v"
                                type="video/mp4"/>
                    </video>
                </td>
            </tr>
        </table>
        <br>


        <strong> Overall Performance </strong><br>
        We implement DiCE framework in both on-policy and off-policy settings
        and compare them with two on-policy baselines PPO, A2C, one off-policy
        baseline SAC and one diversity-encouraging baseline TNB.
        We train our agents in five locomotion tasks in MuJoCo simulator.

        <table width="100%" style="margin: 20pt 0; text-align: center;">
            <tr>
                <td><img src="assets/table.png" width="60%"></td>
            </tr>
        </table>

        <strong> On-policy Setting </strong><br>
        As shown in following figures, in all the five tasks, our method
        achieves better results compared to the baselines PPO and A2C. In the
        above table, we see that in four environments DiCE-PPO achieves a
        substantial improvement over the baselines,
        while in the Hopper-v3 PPO and TNB achieve higher score than DiCE.
        In Hopper-v3, PPO collapses after a long time of training, while DiCE
        maintains its performance until the end of the training, which shows
        that DiCE is robust in training stability.


        <table width="100%" style="margin: 20pt 0; text-align: center;">
            <tr>
                <td><img src="assets/diceppo/ant.png" width="100%"></td>
                <td><img src="assets/diceppo/halfcheetah.png" width="100%"></td>
                <td><img src="assets/diceppo/hopper.png" width="100%"></td>
                <td><img src="assets/diceppo/humanoid.png" width="100%"></td>
                <td><img src="assets/diceppo/walker.png" width="100%"></td>
            </tr>
        </table>


        <strong> Off-policy Setting </strong><br>
        As shown in the table and the following figures, in off-policy setting,
        DiCE-SAC outperforms the SAC baseline in Hopper-v3 and Humanoid-v3 with
        faster convergence while achieves comparable performance in
        HalfCheetah-v3 and Walker2d-v3. In Ant-v3, the DiCE-SAC fails to
        progress compared to SAC. This might because that Ant-v3 environment has
        loose constraints on action and has larger action space, thus the
        structure of diversity is more complex than other environments, making
        the learning of diversity critic harder. We have the similar observation
        for on-policy DiCE when utilizing a diversity value network (please
        refer to the ablation study in paper).

        <table width="100%" style="margin: 20pt 0; text-align: center;">
            <tr>
                <td><img src="assets/dicesac/ant.png" width="100%"></td>
                <td><img src="assets/dicesac/halfcheetah.png" width="100%"></td>
                <td><img src="assets/dicesac/hopper.png" width="100%"></td>
                <td><img src="assets/dicesac/humanoid.png" width="100%"></td>
                <td><img src="assets/dicesac/walker.png" width="100%"></td>
            </tr>
        </table>

        The performance improvements brought by DiCE in on-policy and off-policy
        settings shows the generalization ability of our framework.

    </div>
</div>
</div>
<!-- === Result Section Ends === -->


<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">BibTeX</div>
    <pre>
@article{peng2020nonlocal,
  title={Non-local Policy Optimization via Diversity-regularized Collaborative Exploration},
  author={Peng, Zhenghao and Sun, Hao and Zhou, Bolei},
  journal={arXiv preprint arXiv:2006.07781},
  year={2020}
}</pre>    <br>
    <a href="https://www.bilibili.com/video/BV1cp4y1Q7UT/" target="_blank">bilibili
        link of video</a><br>
    <a href="https://youtu.be/wijDTvxDwnw" target="_blank">YouTube link of
        video</a>

    <!-- BZ: we should give other related work enough credits, -->
    <!--     so please include some most relevant work and leave some comment to summarize work and the difference. -->
    <!--   <div class="ref">Related Work</div>
      <div class="citation">
        <div class="image"><img src="https://via.placeholder.com/300x100"></div>
        <div class="comment">
          <a href="#" target="_blank">
            Authors.
            Paper Title.
            Conference Name & Year.</a><br>
          <b>Comment:</b>
          This is a short comment.
        </div>
      </div>
      <div class="citation">
        <div class="image"><img src="https://via.placeholder.com/300x100"></div>
        <div class="comment">
          <a href="#" target="_blank">
            Authors.
            Paper Title.
            Conference Name & Year.</a><br>
          <b>Comment:</b>
          This is a long comment. This comment is just used to test how long comments can fit the template.
        </div>
      </div>
    </div> -->
    <!-- === Reference Section Ends === -->

</div>
</body>
</html>
